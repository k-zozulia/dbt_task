## Task 1: View vs Table Materialization

### Overview
This demo compares **view** and **table** materializations in dbt using Snowflake's sample dataset (150k customer records).

### Model: `models/materialization_demo.sql`

```sql
{{ config(materialized='table') }}  -- {{ config(materialized='view') }} for view

with src as (
    select * from SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER  -- 150k rows
),
agg as (
    select
        c_nationkey,
        count(*) as customer_count,
        avg(c_acctbal)::decimal(12,2) as avg_balance
    from src
    group by c_nationkey
)

select * from agg  -- 25 rows result
```

---

## Experiment Results (Dec 12, 2025)

| Materialization | Warehouse | Build Time | Bytes Written | Rows Written | Object Created |
|-----------------|-----------|------------|---------------|--------------|----------------|
| **view** | â€” | 181 ms | 0 B | 0 | `materialization_demo_view` |
| **table** | X-Small | 1.2 s | ~28 KB | 25 | `materialization_demo_table` |

---

## Recommendations for use

| Use Case | Choose | Reason |
|----------|--------|--------|
| Frequently queried + expensive logic | **table** | Fast & predictable reads |
| Small/intermediate model | **view** | Zero storage, always fresh |
| Needs real-time data | **view** | Always sees latest source data |
| Used in dashboards / BI tools | **table** | Consistent performance for end users |
| Large joins + repeated downstream use | **table** | Much faster subsequent queries |

---

## Key Findings

### Table Materialization âœ…
- **Build cost:** 1.2s once (during `dbt run`)
- **Query cost:** ~50ms every time (reads stored data)
- **Storage:** ~28 KB (negligible for this size)
- **Best for:** Production models queried frequently

### View Materialization âš ï¸
- **Build cost:** 181ms (just creates SQL definition)
- **Query cost:** ~150ms every time (recomputes aggregation)
- **Storage:** 0 B
- **Best for:** Lightweight transformations, always-fresh data

### Performance Math
If queried 100 times:
- **Table:** 1.2s build + (100 Ã— 0.05s) = 6.2s total
- **View:** 0.18s build + (100 Ã— 0.15s) = 15.18s total

**Table is 2.4Ã— faster** for frequently accessed models.
---

## Conclusion
**Default to `table` materialization** for:
- Mart/analytics models
- Models used in BI tools
- Any model queried more than a few times per day

**Use `view` for:**
- Staging models (simple renaming/casting)
- Models that must reflect real-time source changes
- Intermediate steps in complex pipelines
---

# Task 2: Incremental Orders Model

## Overview
Incremental model that processes only new/updated records from TPCH ORDERS dataset.

## Configuration

```sql
{{ config(
    materialized='incremental',
    unique_key='order_key',
    incremental_strategy='merge'
) }}
```

- **Source:** `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS`
- **Strategy:** Merge (updates existing, inserts new)
- **Records:** 1.5M rows

## Incremental Logic

```sql
{% if is_incremental() %}
    where o_orderdate > (select max(order_date) from {{ this }})
{% endif %}
```

- **First run:** Loads all data (full load)
- **Subsequent runs:** Only new records after max date
- **`{{ this }}`:** References current model table

## Handling Late-Arriving Data

### Problem
Orders updated after initial load are missed because `order_date` doesn't change:
```
Day 1: Order created (status: 'pending')
Day 2: ETL loads order
Day 3: Order updated (status: 'shipped') â† Missed!
```

### Solution: Lookback Period

```sql
{% if is_incremental() %}
    where o_orderdate >= (
        select dateadd(day, -3, max(order_date)) 
        from {{ this }}
    )
{% endif %}
```

**How it works:**
- Reprocess last 3 days of data on each run
- Merge strategy updates changed records via `unique_key`

**Trade-off:** Reprocesses some unchanged data for guaranteed freshness

### Alternative Approaches

| Method | Accuracy | Performance | Complexity |
|--------|----------|-------------|------------|
| Lookback Period | Good | Fast | Simple |
| `updated_at` field | Excellent | Fastest | Medium |
| Full Refresh Weekly | Perfect | Slow | Simple |


# Task 3: Ephemeral Models and Ref Chains

## Overview
This pipeline demonstrates ephemeral materialization in dbt, where intermediate transformations are compiled as CTEs rather than physical database objects.

## Pipeline Architecture

```
orders (source)
    â†“
stg_orders (view)
    â†“
int_orders_with_status (ephemeral) ðŸ‘»
    â†“
int_orders_enriched (ephemeral) ðŸ‘»
    â†“
fct_orders (table)
```

**Key Point:** The two intermediate models don't exist in the database - they're inlined as CTEs in the final model's SQL.

---

## Benefits of Ephemeral Models

| Benefit | Explanation |
|---------|-------------|
| **Clean Database** | No clutter from intermediate transformations |
| **Modular Code** | Each model has single responsibility, easy to maintain |
| **No Permission Issues** | Ephemeral models don't exist, so no access control needed |
| **Single Query Execution** | All logic runs in one optimized query |
| **Version Control Friendly** | Changes are self-contained in model files |

---

## Limitations of Ephemeral Models

| Limitation | Impact | Mitigation |
|------------|--------|------------|
| **Can't query directly** | `SELECT * FROM int_orders_enriched` fails | Change to `view` for debugging |
| **Code duplication** | If referenced by >1 model, CTE is duplicated | Use `view` for shared models |
| **Harder debugging** | Can't inspect intermediate results | Temporarily materialize as view |
| **Large CTE performance** | Complex ephemeral models may not optimize well | Use `table` for expensive transformations |

---

## Lineage Diagram

Visual representation of model dependencies: